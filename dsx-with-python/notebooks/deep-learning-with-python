{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Deep Learning Fundamentals\n\n## Introduction to Deep Learning\n\n### What is a Nueral Network\n\n> For more look at: Micheal Nielson and Andrew Ng\n\nA NN's main function is to receive an input, do some calculations, and based on that solve some sort of problem\n\nNN's can be done for a few different things, such as classification\n\nIt is made up of a series of classifiers layered after one another, this makes use of an **Input Layer**, an **Output Layer**, and a few **Hidden Layers**\n\nThe process of going from Input to Output is known as Forward Propogation\n\nNueral Nets are also known as Multi Layer Perceptrons, each node is not necessarily a perceptron but may be a slightly more complex node\n\nNN's make use of weights and biases to place different importance of inputs. We train a network by comparing the predicted output to the actual output and modify weights and biases in order to train and become more accurate\n\n### Why Deep Learning?\n\nNN's are exceptionally good at finding complex patterns as well as enabling us to train them by making use of GPUs\n\nIf the data has many different inputs NN's tend to become better than other classifiers\n\nWhen we have many features and combinations we need a Deep Net in order to properly classify the data due to the complexity in patterns\n\nDeep nets break down complex patterns into many simpler patterns and combine these\n\nThe problem is that Deep Nets take very long to train, we can however use high speed GPU's to train NN's faster\n\n\n### Different Deep Nets\n\n- Unlabelled Data\n    - RBM\n    - Autoencoder\n- Labelled\n    - Text Processing\n        - RNTN\n        - Recurrent Net\n    - Image Recognition\n        - DBN\n        - Convolutional Net\n    - Object Recognition\n        - Convolutional Net\n        - RNTN\n    - Speech Recognition\n        - Recurrent Net\n- General\n    - Classification\n        - MLP\n        - RELU\n    - Time Series\n        - Recurrent Net\n\n### The Vanishing Gradient\n\nDeep Nets have been around for a long time, but they are really difficult to train with back propogation due to a problem known as the Vanishing Gradient\n\nThe gradient is the rate that the cost will change given a change in weights or biases\n\nWhen a gradient is large, the net will train faster than when the gradient is small\n\nEarly Layers have the smallest gradient but are the most important part as these being poorly trained can result in the later layers being affected\n\nDue to the way the bias multiplication works, Back propogation performs poorly due to the fact that the biases keep getting smaller towards the later layers thus leadning to a smaller and smaller gradient\n\n## Deep Learning Models\n\n> The major breakthrough came after three papers by Hinton, Lecun and Bengio in 2006 and 2007\n\n### Restricted Boltzmann Machines\n\nThe RBM is a shallow, two layer net. Each node is connected to each previous layer, but not to any node on their layer\n\nAn RBM is trained to reconstruct the input data through a series of forward and backward passes\n\nRBM's make use of KL Divergence to train them\n\nRBM Data does not need to be labelled. An RBM makes decisions about what features are important and how they should be combined. RBM is part of a family of NN's known as Autoencoders which are able to extract features\n\n### Deep Belief Nets\n\nA DBN is a combination of RBM's. A DBN is identical to an MLP but is trained in a different way which is the differentiating factor\n\nEvery set of two layers is used trained as an RBM and tunes the entire model simultaneously\n\nTo finish the training process we take a small set of labelled samples which will slightly affect the biases in the net but increase the accuracy\n\n### Convolutional Nets\n\nThe CNN has dominated the Image Recognition space. CNN's were developed by Yann Lecun \n\n> For more detailed information look at Andrej Karpathy's CS231 Notes\n\nA CNN consists of many components\n\nThe first component is the Convolutional Layer, this is used to identify a specific pattern such as an edge, this creates a filter. We use multiple simultaneous filters to look for different patterns\n\nThe net uses Convolution to search for a specific pattern\n\nIn the Convolutoion layer the nuerons does convlution. Each nueron is only connected to **some** input nuerons, not all\n\nThe next two layers are RELU and Pooling. CNN's combine multiple Convolutional Layers, RELU and Pooling layers. The Pooling layers help to reduce the complexity between layers\n\nAt the end there is a Fully Connected net which helps to classify the output data from the Pooling Layer\n\nCNN's are supervised models which mean they require a lot of labelled data which can be difficult to come across\n\n\n### Recurrent Nets\n\n> Jurgen Schmidhuber, Sepp Horchreiter and Alex Graves\n\nThese can be applied to anything from speech recognition to driverless cars\n\nThese networks have a feedback loop in which the output is fed back into the input layer\n\nA recurrent net can receive a sequence and output a sequence\n\nRNN's can be stacked to perform more complex operations\n\nRNN's are dificult to train and result in an extreme vanishing gradient\n\nThere are multiple solutions to this problem, the most popular is to use Gating units like LSTM and GRU\n\nGating helps the net figure out when to remember and forget a specific input\n\nGPU's are the usual tool for training an RNN\n\nFeed Forward Nets output one Value, whereas an RNN can output a sequence of values such as in the case of forecasting\n\n### Autoencoders\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}