{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Deep Learning with TensorFlow Module 3\n\n[Based on this CognitiveClass Course](https://cognitiveclass.ai/courses/deep-learning-tensorflow/)\n\n## Recurrent Neural Nets\n\n### Sequential Data\n\nSequential data produces a problem when trying to make use of traditional feed-forward neural networks\n\nNNs assume that inputs are independent and therefore do not take into consideration the impact of previous data\n\n### Recurrent Neural Networks\n\nRNNs are able to maintain a state or context which allows the model to know what was previously calculated. The model feeds back in previous inputs with the new input in order to help us predict sequential information\n\n\n$$\nh_{new}=tanh(W_h\\cdot h_{prev}+W_x\\cdot x)\n$$\n\nWhere $h_{prev}$ is the context, and the new state $h_{new}$ which will yield an output $y$\n\n$$\ny=W_y\\cdot h_{new}\n$$\n\nRNNs need to keep track of state at any given timepoint and can become computationally expensive. These are also very sensative to changes in parameters and can suffer from a Vanishing or Exploding gradient\n\nOverall these models can be more difficult to train\n\n### Long Short-Term Memory\n\nA standard RNN can be difficult when trying to learn very long sequences\n\nA solution to this is the LSTM model which maintains a strong gradient over relatively long sequences\n\nAn LSTM is contained of a memory cell and 3 logistic gates for writing, reading, and forgeting which control the flow of data\n\nManipulating these gates allows an LSTM to remember the appropriate level of information\n\nWe can also step LSTMs on top of each other to create hierarchial data representations\n\nDuring the training process the network will learn how much old, and new information to remember as well as the weights and biases based on the different levels of state for each gate at each layer\n\n### Language Modeling\n\nLanguage modeling is the process of assigning prbabilities to a sequence of words. We can make use of an RNN to form the context based on previous words and thereafter output the new word\n\nWord embedding is a way to process words by decoding a word to a spefici vector. The vectors for the vocabulary are initialized randomly and then are updated based on the context that the word is the same and hence words used in similar contexts ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}